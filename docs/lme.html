<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Linear Mixed Models | R (BGU course)</title>
  <meta name="description" content="Class notes for the R course at the BGU’s IE&amp;M dept." />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Linear Mixed Models | R (BGU course)" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Class notes for the R course at the BGU’s IE&amp;M dept." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Linear Mixed Models | R (BGU course)" />
  
  <meta name="twitter:description" content="Class notes for the R course at the BGU’s IE&amp;M dept." />
  

<meta name="author" content="Jonathan D. Rosenblatt" />


<meta name="date" content="2020-09-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="glm.html">
<link rel="next" href="multivariate.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<link href="libs/sequences-0.1/sequences.css" rel="stylesheet" />
<script src="libs/sunburst-binding-2.1.1/sunburst.js"></script>
<script src="libs/d3-5.7.0/d3.min.js"></script>
<script src="libs/d3-lasso-0.0.5/d3-lasso.min.js"></script>
<link href="libs/ggiraphjs-0.1.0/styles.css" rel="stylesheet" />
<script src="libs/ggiraphjs-0.1.0/ggiraphjs.min.js"></script>
<script src="libs/girafe-binding-0.6.1/girafe.js"></script>
<script src="libs/plotly-binding-4.9.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.46.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.46.1/plotly-latest.min.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R Course</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Notation Conventions</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>1.1</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#what-r"><i class="fa fa-check"></i><b>2.1</b> What is R?</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#ecosystem"><i class="fa fa-check"></i><b>2.2</b> The R Ecosystem</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#bibliographic-notes"><i class="fa fa-check"></i><b>2.3</b> Bibliographic Notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="basics.html"><a href="basics.html"><i class="fa fa-check"></i><b>3</b> R Basics</a><ul>
<li class="chapter" data-level="3.0.1" data-path="basics.html"><a href="basics.html#other-ides"><i class="fa fa-check"></i><b>3.0.1</b> Other IDEs</a></li>
<li class="chapter" data-level="3.1" data-path="basics.html"><a href="basics.html#file-types"><i class="fa fa-check"></i><b>3.1</b> File types</a></li>
<li class="chapter" data-level="3.2" data-path="basics.html"><a href="basics.html#simple-calculator"><i class="fa fa-check"></i><b>3.2</b> Simple calculator</a></li>
<li class="chapter" data-level="3.3" data-path="basics.html"><a href="basics.html#probability-calculator"><i class="fa fa-check"></i><b>3.3</b> Probability calculator</a></li>
<li class="chapter" data-level="3.4" data-path="basics.html"><a href="basics.html#getting-help"><i class="fa fa-check"></i><b>3.4</b> Getting Help</a></li>
<li class="chapter" data-level="3.5" data-path="basics.html"><a href="basics.html#variable-assignment"><i class="fa fa-check"></i><b>3.5</b> Variable Assignment</a></li>
<li class="chapter" data-level="3.6" data-path="basics.html"><a href="basics.html#missing"><i class="fa fa-check"></i><b>3.6</b> Missing</a></li>
<li class="chapter" data-level="3.7" data-path="basics.html"><a href="basics.html#piping"><i class="fa fa-check"></i><b>3.7</b> Piping</a></li>
<li class="chapter" data-level="3.8" data-path="basics.html"><a href="basics.html#vector-creation-and-manipulation"><i class="fa fa-check"></i><b>3.8</b> Vector Creation and Manipulation</a></li>
<li class="chapter" data-level="3.9" data-path="basics.html"><a href="basics.html#search-paths-and-packages"><i class="fa fa-check"></i><b>3.9</b> Search Paths and Packages</a></li>
<li class="chapter" data-level="3.10" data-path="basics.html"><a href="basics.html#simple-plotting"><i class="fa fa-check"></i><b>3.10</b> Simple Plotting</a></li>
<li class="chapter" data-level="3.11" data-path="basics.html"><a href="basics.html#object-types"><i class="fa fa-check"></i><b>3.11</b> Object Types</a></li>
<li class="chapter" data-level="3.12" data-path="basics.html"><a href="basics.html#data-frames"><i class="fa fa-check"></i><b>3.12</b> Data Frames</a></li>
<li class="chapter" data-level="3.13" data-path="basics.html"><a href="basics.html#exctraction"><i class="fa fa-check"></i><b>3.13</b> Exctraction</a></li>
<li class="chapter" data-level="3.14" data-path="basics.html"><a href="basics.html#augmentations-of-the-data.frame-class"><i class="fa fa-check"></i><b>3.14</b> Augmentations of the data.frame class</a></li>
<li class="chapter" data-level="3.15" data-path="basics.html"><a href="basics.html#data-import-and-export"><i class="fa fa-check"></i><b>3.15</b> Data Import and Export</a><ul>
<li class="chapter" data-level="3.15.1" data-path="basics.html"><a href="basics.html#import-from-web"><i class="fa fa-check"></i><b>3.15.1</b> Import from WEB</a></li>
<li class="chapter" data-level="3.15.2" data-path="basics.html"><a href="basics.html#import-from-clipboard"><i class="fa fa-check"></i><b>3.15.2</b> Import From Clipboard</a></li>
<li class="chapter" data-level="3.15.3" data-path="basics.html"><a href="basics.html#export-as-csv"><i class="fa fa-check"></i><b>3.15.3</b> Export as CSV</a></li>
<li class="chapter" data-level="3.15.4" data-path="basics.html"><a href="basics.html#export-non-csv-files"><i class="fa fa-check"></i><b>3.15.4</b> Export non-CSV files</a></li>
<li class="chapter" data-level="3.15.5" data-path="basics.html"><a href="basics.html#reading-from-text-files"><i class="fa fa-check"></i><b>3.15.5</b> Reading From Text Files</a></li>
<li class="chapter" data-level="3.15.6" data-path="basics.html"><a href="basics.html#writing-data-to-text-files"><i class="fa fa-check"></i><b>3.15.6</b> Writing Data to Text Files</a></li>
<li class="chapter" data-level="3.15.7" data-path="basics.html"><a href="basics.html#xlsx-files"><i class="fa fa-check"></i><b>3.15.7</b> .XLS(X) files</a></li>
<li class="chapter" data-level="3.15.8" data-path="basics.html"><a href="basics.html#massive-files"><i class="fa fa-check"></i><b>3.15.8</b> Massive files</a></li>
<li class="chapter" data-level="3.15.9" data-path="basics.html"><a href="basics.html#databases"><i class="fa fa-check"></i><b>3.15.9</b> Databases</a></li>
</ul></li>
<li class="chapter" data-level="3.16" data-path="basics.html"><a href="basics.html#functions"><i class="fa fa-check"></i><b>3.16</b> Functions</a></li>
<li class="chapter" data-level="3.17" data-path="basics.html"><a href="basics.html#looping"><i class="fa fa-check"></i><b>3.17</b> Looping</a></li>
<li class="chapter" data-level="3.18" data-path="basics.html"><a href="basics.html#apply"><i class="fa fa-check"></i><b>3.18</b> Apply</a></li>
<li class="chapter" data-level="3.19" data-path="basics.html"><a href="basics.html#recursion"><i class="fa fa-check"></i><b>3.19</b> Recursion</a></li>
<li class="chapter" data-level="3.20" data-path="basics.html"><a href="basics.html#strings"><i class="fa fa-check"></i><b>3.20</b> Strings</a></li>
<li class="chapter" data-level="3.21" data-path="basics.html"><a href="basics.html#dates-and-times"><i class="fa fa-check"></i><b>3.21</b> Dates and Times</a><ul>
<li class="chapter" data-level="3.21.1" data-path="basics.html"><a href="basics.html#dates"><i class="fa fa-check"></i><b>3.21.1</b> Dates</a></li>
<li class="chapter" data-level="3.21.2" data-path="basics.html"><a href="basics.html#times"><i class="fa fa-check"></i><b>3.21.2</b> Times</a></li>
<li class="chapter" data-level="3.21.3" data-path="basics.html"><a href="basics.html#lubridate-package"><i class="fa fa-check"></i><b>3.21.3</b> lubridate Package</a></li>
</ul></li>
<li class="chapter" data-level="3.22" data-path="basics.html"><a href="basics.html#complex-objects"><i class="fa fa-check"></i><b>3.22</b> Complex Objects</a></li>
<li class="chapter" data-level="3.23" data-path="basics.html"><a href="basics.html#vectors-and-matrix-products"><i class="fa fa-check"></i><b>3.23</b> Vectors and Matrix Products</a></li>
<li class="chapter" data-level="3.24" data-path="basics.html"><a href="basics.html#rstudio-projects"><i class="fa fa-check"></i><b>3.24</b> RStudio Projects</a></li>
<li class="chapter" data-level="3.25" data-path="basics.html"><a href="basics.html#bibliographic-notes-1"><i class="fa fa-check"></i><b>3.25</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="3.26" data-path="basics.html"><a href="basics.html#practice-yourself"><i class="fa fa-check"></i><b>3.26</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="datatable.html"><a href="datatable.html"><i class="fa fa-check"></i><b>4</b> data.table</a><ul>
<li class="chapter" data-level="4.1" data-path="datatable.html"><a href="datatable.html#make-your-own-variables"><i class="fa fa-check"></i><b>4.1</b> Make your own variables</a></li>
<li class="chapter" data-level="4.2" data-path="datatable.html"><a href="datatable.html#join"><i class="fa fa-check"></i><b>4.2</b> Join</a></li>
<li class="chapter" data-level="4.3" data-path="datatable.html"><a href="datatable.html#reshaping-data"><i class="fa fa-check"></i><b>4.3</b> Reshaping data</a><ul>
<li class="chapter" data-level="4.3.1" data-path="datatable.html"><a href="datatable.html#wide-to-long"><i class="fa fa-check"></i><b>4.3.1</b> Wide to long</a></li>
<li class="chapter" data-level="4.3.2" data-path="datatable.html"><a href="datatable.html#long-to-wide"><i class="fa fa-check"></i><b>4.3.2</b> Long to wide</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="datatable.html"><a href="datatable.html#bibliographic-notes-2"><i class="fa fa-check"></i><b>4.4</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="4.5" data-path="datatable.html"><a href="datatable.html#practice-yourself-1"><i class="fa fa-check"></i><b>4.5</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="eda.html"><a href="eda.html"><i class="fa fa-check"></i><b>5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="5.1" data-path="eda.html"><a href="eda.html#summary-statistics"><i class="fa fa-check"></i><b>5.1</b> Summary Statistics</a><ul>
<li class="chapter" data-level="5.1.1" data-path="eda.html"><a href="eda.html#categorical-data"><i class="fa fa-check"></i><b>5.1.1</b> Categorical Data</a></li>
<li class="chapter" data-level="5.1.2" data-path="eda.html"><a href="eda.html#continous-data"><i class="fa fa-check"></i><b>5.1.2</b> Continous Data</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="eda.html"><a href="eda.html#visualization"><i class="fa fa-check"></i><b>5.2</b> Visualization</a><ul>
<li class="chapter" data-level="5.2.1" data-path="eda.html"><a href="eda.html#categorical-data-1"><i class="fa fa-check"></i><b>5.2.1</b> Categorical Data</a></li>
<li class="chapter" data-level="5.2.2" data-path="eda.html"><a href="eda.html#continuous-data"><i class="fa fa-check"></i><b>5.2.2</b> Continuous Data</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="eda.html"><a href="eda.html#mixed-type-data"><i class="fa fa-check"></i><b>5.3</b> Mixed Type Data</a><ul>
<li class="chapter" data-level="5.3.1" data-path="eda.html"><a href="eda.html#alluvial"><i class="fa fa-check"></i><b>5.3.1</b> Alluvial Diagram</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="eda.html"><a href="eda.html#bibliographic-notes-3"><i class="fa fa-check"></i><b>5.4</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="5.5" data-path="eda.html"><a href="eda.html#practice-yourself-2"><i class="fa fa-check"></i><b>5.5</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lm.html"><a href="lm.html"><i class="fa fa-check"></i><b>6</b> Linear Models</a><ul>
<li class="chapter" data-level="6.1" data-path="lm.html"><a href="lm.html#problem-setup"><i class="fa fa-check"></i><b>6.1</b> Problem Setup</a></li>
<li class="chapter" data-level="6.2" data-path="lm.html"><a href="lm.html#ols-estimation-in-r"><i class="fa fa-check"></i><b>6.2</b> OLS Estimation in R</a></li>
<li class="chapter" data-level="6.3" data-path="lm.html"><a href="lm.html#inference"><i class="fa fa-check"></i><b>6.3</b> Inference</a><ul>
<li class="chapter" data-level="6.3.1" data-path="lm.html"><a href="lm.html#testing-a-hypothesis-on-a-single-coefficient"><i class="fa fa-check"></i><b>6.3.1</b> Testing a Hypothesis on a Single Coefficient</a></li>
<li class="chapter" data-level="6.3.2" data-path="lm.html"><a href="lm.html#constructing-a-confidence-interval-on-a-single-coefficient"><i class="fa fa-check"></i><b>6.3.2</b> Constructing a Confidence Interval on a Single Coefficient</a></li>
<li class="chapter" data-level="6.3.3" data-path="lm.html"><a href="lm.html#multiple-regression"><i class="fa fa-check"></i><b>6.3.3</b> Multiple Regression</a></li>
<li class="chapter" data-level="6.3.4" data-path="lm.html"><a href="lm.html#anova"><i class="fa fa-check"></i><b>6.3.4</b> ANOVA (*)</a></li>
<li class="chapter" data-level="6.3.5" data-path="lm.html"><a href="lm.html#testing-a-hypothesis-on-a-single-contrast"><i class="fa fa-check"></i><b>6.3.5</b> Testing a Hypothesis on a Single Contrast (*)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="lm.html"><a href="lm.html#extra-diagnostics"><i class="fa fa-check"></i><b>6.4</b> Extra Diagnostics</a><ul>
<li class="chapter" data-level="6.4.1" data-path="lm.html"><a href="lm.html#diagnosing-heteroskedasticity"><i class="fa fa-check"></i><b>6.4.1</b> Diagnosing Heteroskedasticity</a></li>
<li class="chapter" data-level="6.4.2" data-path="lm.html"><a href="lm.html#diagnosing-multicolinearity"><i class="fa fa-check"></i><b>6.4.2</b> Diagnosing Multicolinearity</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="lm.html"><a href="lm.html#how-does-r-encode-factor-variables"><i class="fa fa-check"></i><b>6.5</b> How Does R Encode Factor Variables?</a></li>
<li class="chapter" data-level="6.6" data-path="lm.html"><a href="lm.html#bibliographic-notes-4"><i class="fa fa-check"></i><b>6.6</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="6.7" data-path="lm.html"><a href="lm.html#practice-yourself-3"><i class="fa fa-check"></i><b>6.7</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>7</b> Generalized Linear Models</a><ul>
<li class="chapter" data-level="7.1" data-path="glm.html"><a href="glm.html#problem-setup-1"><i class="fa fa-check"></i><b>7.1</b> Problem Setup</a></li>
<li class="chapter" data-level="7.2" data-path="glm.html"><a href="glm.html#logistic-regression"><i class="fa fa-check"></i><b>7.2</b> Logistic Regression</a><ul>
<li class="chapter" data-level="7.2.1" data-path="glm.html"><a href="glm.html#logistic-regression-with-r"><i class="fa fa-check"></i><b>7.2.1</b> Logistic Regression with R</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="glm.html"><a href="glm.html#poisson-regression"><i class="fa fa-check"></i><b>7.3</b> Poisson Regression</a></li>
<li class="chapter" data-level="7.4" data-path="glm.html"><a href="glm.html#extensions"><i class="fa fa-check"></i><b>7.4</b> Extensions</a></li>
<li class="chapter" data-level="7.5" data-path="glm.html"><a href="glm.html#bibliographic-notes-5"><i class="fa fa-check"></i><b>7.5</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="7.6" data-path="glm.html"><a href="glm.html#practice-glm"><i class="fa fa-check"></i><b>7.6</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lme.html"><a href="lme.html"><i class="fa fa-check"></i><b>8</b> Linear Mixed Models</a><ul>
<li class="chapter" data-level="8.1" data-path="lme.html"><a href="lme.html#problem-setup-2"><i class="fa fa-check"></i><b>8.1</b> Problem Setup</a><ul>
<li class="chapter" data-level="8.1.1" data-path="lme.html"><a href="lme.html#non-linear-mixed-models"><i class="fa fa-check"></i><b>8.1.1</b> Non-Linear Mixed Models</a></li>
<li class="chapter" data-level="8.1.2" data-path="lme.html"><a href="lme.html#generalized-linear-mixed-models-glmm"><i class="fa fa-check"></i><b>8.1.2</b> Generalized Linear Mixed Models (GLMM)</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="lme.html"><a href="lme.html#lmms-in-r"><i class="fa fa-check"></i><b>8.2</b> LMMs in R</a><ul>
<li class="chapter" data-level="8.2.1" data-path="lme.html"><a href="lme.html#a-single-random-effect"><i class="fa fa-check"></i><b>8.2.1</b> A Single Random Effect</a></li>
<li class="chapter" data-level="8.2.2" data-path="lme.html"><a href="lme.html#a-full-mixed-model"><i class="fa fa-check"></i><b>8.2.2</b> A Full Mixed-Model</a></li>
<li class="chapter" data-level="8.2.3" data-path="lme.html"><a href="lme.html#sparsity-and-memory-efficiency"><i class="fa fa-check"></i><b>8.2.3</b> Sparsity and Memory Efficiency</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="lme.html"><a href="lme.html#serial"><i class="fa fa-check"></i><b>8.3</b> Serial Correlations in Space/Time</a></li>
<li class="chapter" data-level="8.4" data-path="lme.html"><a href="lme.html#extensions-1"><i class="fa fa-check"></i><b>8.4</b> Extensions</a><ul>
<li class="chapter" data-level="8.4.1" data-path="lme.html"><a href="lme.html#cr-se"><i class="fa fa-check"></i><b>8.4.1</b> Cluster Robust Standard Errors</a></li>
<li class="chapter" data-level="8.4.2" data-path="lme.html"><a href="lme.html#linear-models-for-panel-data"><i class="fa fa-check"></i><b>8.4.2</b> Linear Models for Panel Data</a></li>
<li class="chapter" data-level="8.4.3" data-path="lme.html"><a href="lme.html#testing-hypotheses-on-correlations"><i class="fa fa-check"></i><b>8.4.3</b> Testing Hypotheses on Correlations</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="lme.html"><a href="lme.html#bibliographic-notes-6"><i class="fa fa-check"></i><b>8.5</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="8.6" data-path="lme.html"><a href="lme.html#practice-yourself-4"><i class="fa fa-check"></i><b>8.6</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="multivariate.html"><a href="multivariate.html"><i class="fa fa-check"></i><b>9</b> Multivariate Data Analysis</a><ul>
<li class="chapter" data-level="9.1" data-path="multivariate.html"><a href="multivariate.html#signal-detection"><i class="fa fa-check"></i><b>9.1</b> Signal Detection</a><ul>
<li class="chapter" data-level="9.1.1" data-path="multivariate.html"><a href="multivariate.html#hotellings-t2-test"><i class="fa fa-check"></i><b>9.1.1</b> Hotelling’s T2 Test</a></li>
<li class="chapter" data-level="9.1.2" data-path="multivariate.html"><a href="multivariate.html#various-types-of-signal-to-detect"><i class="fa fa-check"></i><b>9.1.2</b> Various Types of Signal to Detect</a></li>
<li class="chapter" data-level="9.1.3" data-path="multivariate.html"><a href="multivariate.html#simes-test"><i class="fa fa-check"></i><b>9.1.3</b> Simes’ Test</a></li>
<li class="chapter" data-level="9.1.4" data-path="multivariate.html"><a href="multivariate.html#signal-detection-with-r"><i class="fa fa-check"></i><b>9.1.4</b> Signal Detection with R</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="multivariate.html"><a href="multivariate.html#signal-counting"><i class="fa fa-check"></i><b>9.2</b> Signal Counting</a></li>
<li class="chapter" data-level="9.3" data-path="multivariate.html"><a href="multivariate.html#identification"><i class="fa fa-check"></i><b>9.3</b> Signal Identification</a><ul>
<li class="chapter" data-level="9.3.1" data-path="multivariate.html"><a href="multivariate.html#signal-identification-in-r"><i class="fa fa-check"></i><b>9.3.1</b> Signal Identification in R</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="multivariate.html"><a href="multivariate.html#signal-estimation"><i class="fa fa-check"></i><b>9.4</b> Signal Estimation (*)</a></li>
<li class="chapter" data-level="9.5" data-path="multivariate.html"><a href="multivariate.html#bibliographic-notes-7"><i class="fa fa-check"></i><b>9.5</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="9.6" data-path="multivariate.html"><a href="multivariate.html#practice-yourself-5"><i class="fa fa-check"></i><b>9.6</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="supervised.html"><a href="supervised.html"><i class="fa fa-check"></i><b>10</b> Supervised Learning</a><ul>
<li class="chapter" data-level="10.1" data-path="supervised.html"><a href="supervised.html#problem-setup-3"><i class="fa fa-check"></i><b>10.1</b> Problem Setup</a><ul>
<li class="chapter" data-level="10.1.1" data-path="supervised.html"><a href="supervised.html#common-hypothesis-classes"><i class="fa fa-check"></i><b>10.1.1</b> Common Hypothesis Classes</a></li>
<li class="chapter" data-level="10.1.2" data-path="supervised.html"><a href="supervised.html#common-complexity-penalties"><i class="fa fa-check"></i><b>10.1.2</b> Common Complexity Penalties</a></li>
<li class="chapter" data-level="10.1.3" data-path="supervised.html"><a href="supervised.html#unbiased-risk-estimation"><i class="fa fa-check"></i><b>10.1.3</b> Unbiased Risk Estimation</a></li>
<li class="chapter" data-level="10.1.4" data-path="supervised.html"><a href="supervised.html#collecting-the-pieces"><i class="fa fa-check"></i><b>10.1.4</b> Collecting the Pieces</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="supervised.html"><a href="supervised.html#supervised-learning-in-r"><i class="fa fa-check"></i><b>10.2</b> Supervised Learning in R</a><ul>
<li class="chapter" data-level="10.2.1" data-path="supervised.html"><a href="supervised.html#least-squares"><i class="fa fa-check"></i><b>10.2.1</b> Linear Models with Least Squares Loss</a></li>
<li class="chapter" data-level="10.2.2" data-path="supervised.html"><a href="supervised.html#svm"><i class="fa fa-check"></i><b>10.2.2</b> SVM</a></li>
<li class="chapter" data-level="10.2.3" data-path="supervised.html"><a href="supervised.html#neural-nets"><i class="fa fa-check"></i><b>10.2.3</b> Neural Nets</a></li>
<li class="chapter" data-level="10.2.4" data-path="supervised.html"><a href="supervised.html#trees"><i class="fa fa-check"></i><b>10.2.4</b> Classification and Regression Trees (CART)</a></li>
<li class="chapter" data-level="10.2.5" data-path="supervised.html"><a href="supervised.html#k-nearest-neighbour-knn"><i class="fa fa-check"></i><b>10.2.5</b> K-nearest neighbour (KNN)</a></li>
<li class="chapter" data-level="10.2.6" data-path="supervised.html"><a href="supervised.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>10.2.6</b> Linear Discriminant Analysis (LDA)</a></li>
<li class="chapter" data-level="10.2.7" data-path="supervised.html"><a href="supervised.html#naive-bayes"><i class="fa fa-check"></i><b>10.2.7</b> Naive Bayes</a></li>
<li class="chapter" data-level="10.2.8" data-path="supervised.html"><a href="supervised.html#random-forrest"><i class="fa fa-check"></i><b>10.2.8</b> Random Forrest</a></li>
<li class="chapter" data-level="10.2.9" data-path="supervised.html"><a href="supervised.html#boosting"><i class="fa fa-check"></i><b>10.2.9</b> Boosting</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="supervised.html"><a href="supervised.html#bibliographic-notes-8"><i class="fa fa-check"></i><b>10.3</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="10.4" data-path="supervised.html"><a href="supervised.html#practice-yourself-6"><i class="fa fa-check"></i><b>10.4</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="plotting.html"><a href="plotting.html"><i class="fa fa-check"></i><b>11</b> Plotting</a><ul>
<li class="chapter" data-level="11.1" data-path="plotting.html"><a href="plotting.html#the-graphics-system"><i class="fa fa-check"></i><b>11.1</b> The graphics System</a><ul>
<li class="chapter" data-level="11.1.1" data-path="plotting.html"><a href="plotting.html#using-existing-plotting-functions"><i class="fa fa-check"></i><b>11.1.1</b> Using Existing Plotting Functions</a></li>
<li class="chapter" data-level="11.1.2" data-path="plotting.html"><a href="plotting.html#exporting-a-plot"><i class="fa fa-check"></i><b>11.1.2</b> Exporting a Plot</a></li>
<li class="chapter" data-level="11.1.3" data-path="plotting.html"><a href="plotting.html#fancy"><i class="fa fa-check"></i><b>11.1.3</b> Fancy graphics Examples</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="plotting.html"><a href="plotting.html#the-ggplot2-system"><i class="fa fa-check"></i><b>11.2</b> The ggplot2 System</a><ul>
<li class="chapter" data-level="11.2.1" data-path="plotting.html"><a href="plotting.html#extensions-of-the-ggplot2-system"><i class="fa fa-check"></i><b>11.2.1</b> Extensions of the ggplot2 System</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="plotting.html"><a href="plotting.html#interactive-graphics"><i class="fa fa-check"></i><b>11.3</b> Interactive Graphics</a><ul>
<li class="chapter" data-level="11.3.1" data-path="plotting.html"><a href="plotting.html#plotly"><i class="fa fa-check"></i><b>11.3.1</b> Plotly</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="plotting.html"><a href="plotting.html#other-r-interfaces-to-javascript-plotting"><i class="fa fa-check"></i><b>11.4</b> Other R Interfaces to JavaScript Plotting</a></li>
<li class="chapter" data-level="11.5" data-path="plotting.html"><a href="plotting.html#bibliographic-notes-9"><i class="fa fa-check"></i><b>11.5</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="11.6" data-path="plotting.html"><a href="plotting.html#practice-yourself-7"><i class="fa fa-check"></i><b>11.6</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="report.html"><a href="report.html"><i class="fa fa-check"></i><b>12</b> Reports</a><ul>
<li class="chapter" data-level="12.1" data-path="report.html"><a href="report.html#knitr"><i class="fa fa-check"></i><b>12.1</b> knitr</a><ul>
<li class="chapter" data-level="12.1.1" data-path="report.html"><a href="report.html#installation"><i class="fa fa-check"></i><b>12.1.1</b> Installation</a></li>
<li class="chapter" data-level="12.1.2" data-path="report.html"><a href="report.html#pandoc-markdown"><i class="fa fa-check"></i><b>12.1.2</b> Pandoc Markdown</a></li>
<li class="chapter" data-level="12.1.3" data-path="report.html"><a href="report.html#rmarkdown"><i class="fa fa-check"></i><b>12.1.3</b> Rmarkdown</a></li>
<li class="chapter" data-level="12.1.4" data-path="report.html"><a href="report.html#bibtex"><i class="fa fa-check"></i><b>12.1.4</b> BibTex</a></li>
<li class="chapter" data-level="12.1.5" data-path="report.html"><a href="report.html#compiling"><i class="fa fa-check"></i><b>12.1.5</b> Compiling</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="report.html"><a href="report.html#bookdown"><i class="fa fa-check"></i><b>12.2</b> bookdown</a></li>
<li class="chapter" data-level="12.3" data-path="report.html"><a href="report.html#shiny"><i class="fa fa-check"></i><b>12.3</b> Shiny</a><ul>
<li class="chapter" data-level="12.3.1" data-path="report.html"><a href="report.html#installation-1"><i class="fa fa-check"></i><b>12.3.1</b> Installation</a></li>
<li class="chapter" data-level="12.3.2" data-path="report.html"><a href="report.html#the-basics-of-shiny"><i class="fa fa-check"></i><b>12.3.2</b> The Basics of Shiny</a></li>
<li class="chapter" data-level="12.3.3" data-path="report.html"><a href="report.html#beyond-the-basics"><i class="fa fa-check"></i><b>12.3.3</b> Beyond the Basics</a></li>
<li class="chapter" data-level="12.3.4" data-path="report.html"><a href="report.html#shinydashboard"><i class="fa fa-check"></i><b>12.3.4</b> shinydashboard</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="report.html"><a href="report.html#flexdashboard"><i class="fa fa-check"></i><b>12.4</b> flexdashboard</a></li>
<li class="chapter" data-level="12.5" data-path="report.html"><a href="report.html#bibliographic-notes-10"><i class="fa fa-check"></i><b>12.5</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="12.6" data-path="report.html"><a href="report.html#practice-yourself-8"><i class="fa fa-check"></i><b>12.6</b> Practice Yourself</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">R (BGU course)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lme" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Linear Mixed Models</h1>

<div class="example">
<span id="exm:dependence" class="example"><strong>Example 8.1  (Dependent Samples on the Mean)  </strong></span>Consider inference on a population’s mean.
Supposedly, more observations imply more information. This, however, is not the case if samples are completely dependent. More observations do not add any new information.
From this example one may think that dependence reduces information. This is a false intuition: negative correlations imply oscillations about the mean, so they are actually more informative on the mean than independent observations.
</div>


<div class="example">
<span id="exm:repeated-measures" class="example"><strong>Example 8.2  (Repeated Measures)  </strong></span>Consider a prospective study, i.e., data that originates from selecting a set of subjects and making measurements on them over time.
Also assume that some subjects received some treatment, and other did not.
When we want to infer on the population from which these subjects have been sampled, we need to recall that some series of observations came from the same subject.
If we were to ignore the subject of origin, and treat each observation as an independent sample point, we will think we have more information on treatment effects than we actually do, i.e., we will have a false sense of security in our inference.
</div>

<p>Sources of variability, i.e. noise, are known in the statistical literature as “random effects”.
Specifying these sources determines the correlation structure in our measurements.
In the simplest linear models of Chapter <a href="lm.html#lm">6</a>, we thought of the variability as originating from measurement error, thus independent of anything else.
No-correlation, and fixed variability is known as <em>sphericity</em>.
Sphericity is of great mathematical convenience, but quite often, unrealistic.</p>
<p>The effects we want to infer on are assumingly non-random, and known “fixed-effects”.
Sources of variability in our measurements, known as “random-effects” are usually not the object of interest.
A model which has both random-effects, and fixed-effects, is known as a “mixed effects” model.
If the model is also linear, it is known as a <em>linear mixed model</em> (LMM).
Here are some examples where LMMs arise.</p>

<div class="example">
<span id="exm:fixed-effects" class="example"><strong>Example 8.3  (Fixed and Random Machine Effect)  </strong></span>Consider a problem from industrial process control: testing for a change in diamteters of manufactured bottle caps.
We want to study the fixed effect of time: before versus after.
Bottle caps are produced by several machines.
Clearly there is variablity in the diameters within-machine and between-machines.
Given a sample of bottle caps from many machines, we could standardize measurements by removing each machine’s average.
This implies we treat machines as fixed effects, subtract them, and consider within-machine variability is the only source of variability. The subtraction of the machine effect, removed information on between-machine variability.<br />
Alternatively, we could consider between-machine variability as another source of uncertainty when inferring on the temporal fixed effect. In which case, would not subtract the machine-effect, bur rather, treat it as a random-effect, in the LMM framework.
</div>


<div class="example">
<span id="exm:random-effects" class="example"><strong>Example 8.4  (Fixed and Random Subject Effect)  </strong></span>Consider an experimenal design where each subject is given 2 types of diets, and his health condition is recorded.
We could standardize over subjects by removing the subject-wise average, before comparing diets.
This is what a paired (t-)test does.
This also implies the within-subject variability is the only source of variability we care about.
Alternatively, for inference on the population of “all subjects” we need to adress the between-subject variability, and not only the within-subject variability.
</div>

<p>The unifying theme of the above examples, is that the variability in our data has several sources.
Which are the sources of variability that need to concern us?
This is a delicate matter which depends on your goals.
As a rule of thumb, we will suggest the following view:
<strong>If information of an effect will be available at the time of prediction, treat it as a fixed effect. If it is not, treat it as a random-effect.</strong></p>
<p>LMMs are so fundamental, that they have earned many names:</p>
<ul>
<li><p><strong>Mixed Effects</strong>:
Because we may have both <em>fixed effects</em> we want to estimate and remove, and <em>random effects</em> which contribute to the variability to infer against.</p></li>
<li><p><strong>Variance Components</strong>:
Because as the examples show, variance has more than a single source (like in the Linear Models of Chapter <a href="lm.html#lm">6</a>).</p></li>
<li><p><strong>Hierarchical Models</strong>:
Because as Example <a href="lme.html#exm:random-effects">8.4</a> demonstrates, we can think of the sampling as hierarchical– first sample a subject, and then sample its response.</p></li>
<li><p><strong>Multilevel Analysis</strong>:
For the same reasons it is also known as Hierarchical Models.</p></li>
<li><p><strong>Repeated Measures</strong>:
Because we make several measurements from each unit, like in Example <a href="lme.html#exm:random-effects">8.4</a>.</p></li>
<li><p><strong>Longitudinal Data</strong>:
Because we follow units over time, like in Example <a href="lme.html#exm:random-effects">8.4</a>.</p></li>
<li><p><strong>Panel Data</strong>:
Is the term typically used in econometric for such longitudinal data.</p></li>
</ul>
<p>Whether we are aiming to infer on a generative model’s parameters, or to make predictions, there is no “right” nor “wrong” approach. Instead, there is always some implied measure of error, and an algorithm may be good, or bad, with respect to this measure (think of false and true positives, for instance).
This is why we care about dependencies in the data: ignoring the dependence structure will probably yield inefficient algorithms.
Put differently, if we ignore the statistical dependence in the data we will probably me making more errors than possible/optimal.</p>
<p>We now emphasize:</p>
<ol style="list-style-type: decimal">
<li><p>Like in previous chapters, by “model” we refer to the assumed generative distribution, i.e., the sampling distribution.</p></li>
<li><p>In a LMM we specify the dependence structure via the hierarchy in the sampling scheme E.g. caps within machine, students within class, etc.
Not all dependency models can be specified in this way!
Dependency structures that are not hierarchical include temporal dependencies (<a href="https://en.wikipedia.org/wiki/Autoregressive_model">AR</a>, <a href="https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average">ARIMA</a>, <a href="https://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity">ARCH</a> and GARCH), <a href="https://en.wikipedia.org/wiki/Spatial_dependence">spatial</a>, <a href="https://en.wikipedia.org/wiki/Markov_chain">Markov Chains</a>, and more.
To specify dependency structures that are no hierarchical, see Chapter 8 in (the excellent) <span class="citation">Weiss (<a href="#ref-weiss2005modeling">2005</a>)</span>.</p></li>
<li><p>If you are using LMMs for predictions, and not for inference on the fixed effects or variance components, then see the Supervised Learning Chapter <a href="supervised.html#supervised">10</a>.
Also recall that machine learning from non-independent observations (such as LMMs) is a delicate matter.</p></li>
</ol>
<div id="problem-setup-2" class="section level2">
<h2><span class="header-section-number">8.1</span> Problem Setup</h2>
<p>We denote an outcome with <span class="math inline">\(y\)</span> and assume its sampling distribution is given by
<span class="math display" id="eq:mixed-model">\[\begin{align}
  y|x,z = x&#39;\beta + z&#39;u + \varepsilon
  \tag{8.1}  
\end{align}\]</span>
where <span class="math inline">\(x\)</span> are the factors with (fixed) effects we want to study, and<span class="math inline">\(\beta\)</span> denotes these effects.
The factors <span class="math inline">\(z\)</span>, with effects <span class="math inline">\(u\)</span>, merely contribute to variability in <span class="math inline">\(y|x\)</span>.</p>
<p>In our repeated measures example (<a href="lme.html#exm:repeated-measures">8.2</a>) the treatment is a fixed effect, and the subject is a random effect.
In our bottle-caps example (<a href="lme.html#exm:fixed-effects">8.3</a>) the time (before vs. after) is a fixed effect, and the machines may be either a fixed or a random effect (depending on the purpose of inference).
In our diet example (<a href="lme.html#exm:random-effects">8.4</a>) the diet is the fixed effect and the subject is a random effect.</p>
<p>Notice that we state <span class="math inline">\(y|x,z\)</span> merely as a convenient way to do inference on <span class="math inline">\(y|x\)</span>.
We could, instead, specify <span class="math inline">\(Var[y|x]\)</span> directly.
The second approach seems less convenient.
This is the power of LMMs!
We specify the covariance not via the matrix <span class="math inline">\(Var[z&#39;u|x]\)</span>, or <span class="math inline">\(Var[y|x]\)</span>, but rather via the sampling hierarchy.</p>
<p>Given a sample of <span class="math inline">\(n\)</span> observations <span class="math inline">\((y_i,x_i,z_i)\)</span> from model <a href="lme.html#eq:mixed-model">(8.1)</a>, we will want to estimate <span class="math inline">\((\beta,u)\)</span>.
Under some assumption on the distribution of <span class="math inline">\(\varepsilon\)</span> and <span class="math inline">\(z\)</span>, we can use <em>maximum likelihood</em> (ML).
In the context of LMMs, however, ML is typically replaced with <em>restricted maximum likelihood</em> (ReML), because it returns unbiased estimates of <span class="math inline">\(Var[y|x]\)</span> and ML does not.</p>
<div id="non-linear-mixed-models" class="section level3">
<h3><span class="header-section-number">8.1.1</span> Non-Linear Mixed Models</h3>
<p>The idea of random-effects can also be extended to non-linear mean models.
Formally, this means that <span class="math inline">\(y|x,z=f(x,z,\varepsilon)\)</span> for some non-linear <span class="math inline">\(f\)</span>.
This is known as <em>non-linear-mixed-models</em>, which will not be discussed in this text.</p>
</div>
<div id="generalized-linear-mixed-models-glmm" class="section level3">
<h3><span class="header-section-number">8.1.2</span> Generalized Linear Mixed Models (GLMM)</h3>
<p>You can marry the ideas of random effects, with non-linear link functions, and non-Gaussian distribution of the response.
These are known as <em>Generalized Linear Mixed Models</em> (GLMM), which will not be discussed in this text.</p>
</div>
</div>
<div id="lmms-in-r" class="section level2">
<h2><span class="header-section-number">8.2</span> LMMs in R</h2>
<p>We will fit LMMs with the <code>lme4::lmer</code> function.
The <strong>lme4</strong> is an excellent package, written by the mixed-models Guru <a href="http://www.stat.wisc.edu/~bates/">Douglas Bates</a>.
We start with a small simulation demonstrating the importance of acknowledging your sources of variability.
Our demonstration consists of fitting a linear model that assumes independence, when data is clearly dependent.</p>
<pre class="sourceCode r"><code class="sourceCode r">n.groups &lt;-<span class="st"> </span><span class="dv">4</span> <span class="co"># number of groups</span>
n.repeats &lt;-<span class="st"> </span><span class="dv">2</span> <span class="co"># samples per group</span>
groups &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>n.groups, <span class="dt">each=</span>n.repeats) <span class="op">%&gt;%</span><span class="st"> </span>as.factor
n &lt;-<span class="st"> </span><span class="kw">length</span>(groups)
z0 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n.groups, <span class="dv">0</span>, <span class="dv">10</span>) 
(z &lt;-<span class="st"> </span>z0[<span class="kw">as.numeric</span>(groups)]) <span class="co"># generate and inspect random group effects</span></code></pre>
<pre><code>## [1]  6.8635182  6.8635182  8.2853917  8.2853917  0.6861244  0.6861244
## [7] -2.4415951 -2.4415951</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">epsilon &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n,<span class="dv">0</span>,<span class="dv">1</span>) <span class="co"># generate measurement error</span>

beta0 &lt;-<span class="st"> </span><span class="dv">2</span> <span class="co"># this is the actual parameter of interest! The global mean.</span>

y &lt;-<span class="st"> </span>beta0 <span class="op">+</span><span class="st"> </span>z <span class="op">+</span><span class="st"> </span>epsilon <span class="co"># sample from an LMM</span></code></pre>
<p>We can now fit the linear and LMM.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fit a linear model assuming independence</span>
lm<span class="fl">.5</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span><span class="dv">1</span>)  

<span class="co"># fit a mixed-model that deals with the group dependence</span>
<span class="kw">library</span>(lme4)
lme<span class="fl">.5</span> &lt;-<span class="st"> </span><span class="kw">lmer</span>(y<span class="op">~</span><span class="dv">1</span><span class="op">|</span>groups) </code></pre>
<p>The summary of the linear model</p>
<pre class="sourceCode r"><code class="sourceCode r">summary.lm<span class="fl">.5</span> &lt;-<span class="st"> </span><span class="kw">summary</span>(lm<span class="fl">.5</span>)
summary.lm<span class="fl">.5</span></code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ 1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.2932 -3.6148  0.5154  3.9928  5.1632 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)    5.449      1.671   3.261   0.0138 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.726 on 7 degrees of freedom</code></pre>
<p>The summary of the LMM</p>
<pre class="sourceCode r"><code class="sourceCode r">summary.lme<span class="fl">.5</span> &lt;-<span class="st"> </span><span class="kw">summary</span>(lme<span class="fl">.5</span>)
summary.lme<span class="fl">.5</span></code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: y ~ 1 | groups
## 
## REML criterion at convergence: 29.6
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -1.08588 -0.61820  0.05879  0.53321  1.03325 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  groups   (Intercept) 25.6432  5.0639  
##  Residual              0.3509  0.5924  
## Number of obs: 8, groups:  groups, 4
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)    5.449      2.541   2.145</code></pre>
<p>Look at the standard error of the global mean, i.e., the intercept:
for <code>lm</code> it is 1.671, and for <code>lme</code> it is 2.541.
Why this difference?
Because <code>lm</code> treats the group effect as fixed, while the mixed model treats the group effect as a source of noise/uncertainty.
Inference using <code>lm</code> underestimates our uncertainty in the estimated population mean (<span class="math inline">\(\beta_0\)</span>).
This is that false-sense of security we may have when ignoring correlations.</p>
<div id="relation-to-paired-t-test" class="section level4">
<h4><span class="header-section-number">8.2.0.1</span> Relation to Paired t-test</h4>
<p>Recall the paired t-test.
Our two-sample–per-group example of the LMM is awfully similar to a paired t-test.
It would be quite troubling if the well-known t-test and the oh-so-powerful LMM would lead to diverging conclusions.
In the previous, we inferred on the global mean; a quantity that cancels out when pairing. For a fair comparison, let’s infer on some temporal effect.
Compare the t-statistic below, to the <code>t value</code> in the summary of <code>lme.6</code>.
Luckily, as we demonstrate, the paired t-test and the LMM are equivalent.
So if you follow authors like <span class="citation">Barr et al. (<a href="#ref-barr2013random">2013</a>)</span> that recommend LMMs instead of pairing, remember, these things are sometimes equivalent.</p>
<pre class="sourceCode r"><code class="sourceCode r">time.fixed.effect &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">c</span>(<span class="st">&#39;Before&#39;</span>,<span class="st">&#39;After&#39;</span>), <span class="dt">times=</span><span class="dv">4</span>) <span class="op">%&gt;%</span><span class="st"> </span>factor
<span class="kw">head</span>(<span class="kw">cbind</span>(y,groups,time.fixed.effect))</code></pre>
<pre><code>##              y groups time.fixed.effect
## [1,]  9.076626      1                 2
## [2,]  8.145687      1                 1
## [3,] 10.611710      2                 2
## [4,] 10.535547      2                 1
## [5,]  2.526772      3                 2
## [6,]  3.782050      3                 1</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">lme<span class="fl">.6</span> &lt;-<span class="st"> </span><span class="kw">lmer</span>(y<span class="op">~</span>time.fixed.effect<span class="op">+</span>(<span class="dv">1</span><span class="op">|</span>groups)) 

<span class="kw">coef</span>(<span class="kw">summary</span>(lme<span class="fl">.6</span>))</code></pre>
<pre><code>##                           Estimate Std. Error    t value
## (Intercept)              5.5544195  2.5513561  2.1770460
## time.fixed.effectBefore -0.2118132  0.4679384 -0.4526518</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(y<span class="op">~</span>time.fixed.effect, <span class="dt">paired=</span><span class="ot">TRUE</span>)<span class="op">$</span>statistic</code></pre>
<pre><code>##         t 
## 0.4526514</code></pre>
</div>
<div id="a-single-random-effect" class="section level3">
<h3><span class="header-section-number">8.2.1</span> A Single Random Effect</h3>
<p>We will use the <code>Dyestuff</code> data from the <strong>lme4</strong> package, which encodes the yield, in grams, of a coloring solution (<code>dyestuff</code>), produced in 6 batches using 5 different preparations.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(Dyestuff, <span class="dt">package=</span><span class="st">&#39;lme4&#39;</span>)
<span class="kw">attach</span>(Dyestuff)
<span class="kw">head</span>(Dyestuff)</code></pre>
<pre><code>##   Batch Yield
## 1     A  1545
## 2     A  1440
## 3     A  1440
## 4     A  1520
## 5     A  1580
## 6     B  1540</code></pre>
<p>And visually</p>
<pre class="sourceCode r"><code class="sourceCode r">lattice<span class="op">::</span><span class="kw">dotplot</span>(Yield<span class="op">~</span>Batch)</code></pre>
<p><img src="Rcourse_files/figure-html/unnamed-chunk-199-1.png" width="50%" /></p>
<p>The plot confirms that <code>Yield</code> varies between <code>Batch</code>s.
We do not want to study this batch effect, but we want our inference to apply to new, unseen, batches<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a>.
We thus need to account for the two sources of variability when inferring on the (global) mean: the within-batch variability, and the between-batch variability
We thus fit a mixed model, with an intercept and random batch effect.</p>
<pre class="sourceCode r"><code class="sourceCode r">lme<span class="fl">.1</span>&lt;-<span class="st"> </span><span class="kw">lmer</span>( Yield <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">|</span>Batch)  , Dyestuff )
<span class="kw">summary</span>(lme<span class="fl">.1</span>)</code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: Yield ~ 1 + (1 | Batch)
##    Data: Dyestuff
## 
## REML criterion at convergence: 319.7
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -1.4117 -0.7634  0.1418  0.7792  1.8296 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  Batch    (Intercept) 1764     42.00   
##  Residual             2451     49.51   
## Number of obs: 30, groups:  Batch, 6
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  1527.50      19.38    78.8</code></pre>
<p>Things to note:</p>
<ul>
<li>The syntax <code>Yield ~ (1|Batch)</code> tells <code>lme4::lmer</code> to fit a model with a global intercept (<code>1</code>) and a random Batch effect (<code>1|Batch</code>). The <code>|</code> operator is the cornerstone of random effect modeling with <code>lme4::lmer</code>.</li>
<li><code>1+</code> isn’t really needed. <code>lme4::lmer</code>, like <code>stats::lm</code> adds it be default. We put it there to remind you it is implied.</li>
<li>As usual, <code>summary</code> is content aware and has a different behavior for <code>lme</code> class objects.</li>
<li>The output distinguishes between random effects (<span class="math inline">\(u\)</span>), a source of variability, and fixed effect (<span class="math inline">\(\beta\)</span>), which we want to study. The mean of the random effect is not reported because it is unassumingly 0.</li>
<li>Were we not interested in standard errors, <code>lm(Yield ~ Batch)</code> would have returned the same (fixed) effects estimates.</li>
</ul>
<p>Some utility functions let us query the <code>lme</code> object.
The function <code>coef</code> will work, but will return a cumbersome output. Better use <code>fixef</code> to extract the fixed effects, and <code>ranef</code> to extract the random effects.
The model matrix (of the fixed effects alone), can be extracted with <code>model.matrix</code>, and predictions with <code>predict</code>.</p>
</div>
<div id="a-full-mixed-model" class="section level3">
<h3><span class="header-section-number">8.2.2</span> A Full Mixed-Model</h3>
<p>In the <code>sleepstudy</code> data, we recorded the reaction times to a series of tests (<code>Reaction</code>), after various subject (<code>Subject</code>) underwent various amounts of sleep deprivation (<code>Day</code>).</p>
<p><img src="Rcourse_files/figure-html/unnamed-chunk-200-1.png" width="50%" /></p>
<p>We now want to estimate the (fixed) effect of the days of sleep deprivation on response time, while allowing each subject to have his/hers own effect.
Put differently, we want to estimate a <em>random slope</em> for the effect of <code>day</code>.
The fixed <code>Days</code> effect can be thought of as the average slope over subjects.</p>
<pre class="sourceCode r"><code class="sourceCode r">lme<span class="fl">.3</span> &lt;-<span class="st"> </span><span class="kw">lmer</span> ( Reaction <span class="op">~</span><span class="st"> </span>Days <span class="op">+</span><span class="st"> </span>( Days <span class="op">|</span><span class="st"> </span>Subject ) , <span class="dt">data=</span> sleepstudy )</code></pre>
<p>Things to note:</p>
<ul>
<li><code>~Days</code> specifies the fixed effect.</li>
<li>We used the <code>(Days|Subject)</code> syntax to tell <code>lme4::lmer</code> we want to fit the model <code>~Days</code> within each subject. Just like when modeling with <code>stats::lm</code>, <code>(Days|Subject)</code> is interpreted as <code>(1+Days|Subject)</code>, so we get a random intercept and slope, per subject.</li>
<li>Were we not interested in standard errors, <code>stats::lm(Reaction~Days*Subject)</code> would have returned (almost) the same effects. Why “almost”? See below…</li>
</ul>
<p>The fixed day effect is:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">fixef</span>(lme<span class="fl">.3</span>)</code></pre>
<pre><code>## (Intercept)        Days 
##   251.40510    10.46729</code></pre>
<p>The variability in the average response (intercept) and day effect is</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ranef</span>(lme<span class="fl">.3</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">lapply</span>(head)</code></pre>
<pre><code>## $Subject
##     (Intercept)       Days
## 308    2.257533  9.1992737
## 309  -40.394272 -8.6205161
## 310  -38.956354 -5.4495796
## 330   23.688870 -4.8141448
## 331   22.258541 -3.0696766
## 332    9.038763 -0.2720535</code></pre>
<p>Did we really need the whole <code>lme</code> machinery to fit a within-subject linear regression and then average over subjects?
The short answer is that if we have a enough data for fitting each subject with it’s own <code>lm</code>, we don’t need <code>lme</code>.
The longer answer is that the assumptions on the distribution of random effect, namely, that they are normally distributed, allow us to pool information from one subject to another.
In the words of John Tukey: “we borrow strength over subjects”.
If the normality assumption is true, this is very good news.
If, on the other hand, you have a lot of samples per subject, and you don’t need to “borrow strength” from one subject to another, you can simply fit within-subject linear models without the mixed-models machinery.
This will avoid any assumptions on the distribution of effects over subjects.
For a full discussion of the pro’s and con’s of hierarchical mixed models, consult our Bibliographic Notes.</p>
<p>To demonstrate the “strength borrowing”, here is a comparison of the lme, versus the effects of fitting a linear model to each subject separately.</p>
<p><img src="Rcourse_files/figure-html/unnamed-chunk-203-1.png" width="50%" /></p>
<p>Here is a comparison of the random-day effect from <code>lme</code> versus a subject-wise linear model. They are not the same.</p>
<p><img src="Rcourse_files/figure-html/unnamed-chunk-204-1.png" width="50%" /></p>
</div>
<div id="sparsity-and-memory-efficiency" class="section level3">
<h3><span class="header-section-number">8.2.3</span> Sparsity and Memory Efficiency</h3>
<p>In Chapter <a href="#sparse"><strong>??</strong></a> we discuss how to efficiently represent matrices in memory.
At this point we can already hint that the covariance matrices implied by LMMs are sparse. This fact is exploited in the <strong>lme4</strong> package, making it very efficient computationally.</p>
</div>
</div>
<div id="serial" class="section level2">
<h2><span class="header-section-number">8.3</span> Serial Correlations in Space/Time</h2>
<p>As previously stated, a hierarchical model of the type <span class="math inline">\(y=x&#39;\beta+z&#39;u+\epsilon\)</span> is a very convenient way to state the correlations of <span class="math inline">\(y|x\)</span> instead of specifying the matrix <span class="math inline">\(Var[z&#39;u+\epsilon|x]\)</span> for various <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span>.
The hierarchical sampling scheme implies correlations in blocks.
What if correlations do not have a block structure?
Temporal data or spatial data, for instance, tend to present correlations that decay smoothly in time/space.
These correlations cannot be represented via a hierarchical sampling scheme.</p>
<p>One way to go about, is to find a dedicated package for space/time data.
For instance, in the <a href="https://cran.r-project.org/web/views/SpatioTemporal.html">Spatio-Temporal Data</a> task view, or the <a href="https://cran.r-project.org/web/views/Environmetrics.html">Ecological and Environmental</a> task view.</p>
<p>Instead, we will show how to solve this matter using the <strong>nlme</strong> package.
This is because <strong>nlme</strong> allows to compound the blocks of covariance of LMMs, with the smoothly decaying covariances of space/time models.</p>
<p>We now use an example from the help of <code>nlme::corAR1</code>.
The <code>nlme::Ovary</code> data is panel data of number of ovarian follicles in different mares (female horse), at various times.<br />
We fit a model with a random <code>Mare</code> effect, and correlations that decay geometrically in time.
In the time-series literature, this is known as an <em>auto-regression of order 1</em> model, or AR(1), in short.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(nlme)
<span class="kw">head</span>(nlme<span class="op">::</span>Ovary)</code></pre>
<pre><code>## Grouped Data: follicles ~ Time | Mare
##   Mare        Time follicles
## 1    1 -0.13636360        20
## 2    1 -0.09090910        15
## 3    1 -0.04545455        19
## 4    1  0.00000000        16
## 5    1  0.04545455        13
## 6    1  0.09090910        10</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">fm1Ovar.lme &lt;-<span class="st"> </span>nlme<span class="op">::</span><span class="kw">lme</span>(<span class="dt">fixed=</span>follicles <span class="op">~</span><span class="st"> </span><span class="kw">sin</span>(<span class="dv">2</span><span class="op">*</span>pi<span class="op">*</span>Time) <span class="op">+</span><span class="st"> </span><span class="kw">cos</span>(<span class="dv">2</span><span class="op">*</span>pi<span class="op">*</span>Time), 
                   <span class="dt">data =</span> Ovary, 
                   <span class="dt">random =</span> <span class="kw">pdDiag</span>(<span class="op">~</span><span class="kw">sin</span>(<span class="dv">2</span><span class="op">*</span>pi<span class="op">*</span>Time)), 
                   <span class="dt">correlation=</span><span class="kw">corAR1</span>() )
<span class="kw">summary</span>(fm1Ovar.lme)</code></pre>
<pre><code>## Linear mixed-effects model fit by REML
##  Data: Ovary 
##        AIC     BIC   logLik
##   1563.448 1589.49 -774.724
## 
## Random effects:
##  Formula: ~sin(2 * pi * Time) | Mare
##  Structure: Diagonal
##         (Intercept) sin(2 * pi * Time) Residual
## StdDev:    2.858385           1.257977 3.507053
## 
## Correlation Structure: AR(1)
##  Formula: ~1 | Mare 
##  Parameter estimate(s):
##       Phi 
## 0.5721866 
## Fixed effects: follicles ~ sin(2 * pi * Time) + cos(2 * pi * Time) 
##                        Value Std.Error  DF   t-value p-value
## (Intercept)        12.188089 0.9436602 295 12.915760  0.0000
## sin(2 * pi * Time) -2.985297 0.6055968 295 -4.929513  0.0000
## cos(2 * pi * Time) -0.877762 0.4777821 295 -1.837159  0.0672
##  Correlation: 
##                    (Intr) s(*p*T
## sin(2 * pi * Time)  0.000       
## cos(2 * pi * Time) -0.123  0.000
## 
## Standardized Within-Group Residuals:
##         Min          Q1         Med          Q3         Max 
## -2.34910093 -0.58969626 -0.04577893  0.52931186  3.37167486 
## 
## Number of Observations: 308
## Number of Groups: 11</code></pre>
<p>Things to note:</p>
<ul>
<li>The fitting is done with the <code>nlme::lme</code> function, and not <code>lme4::lmer</code>.</li>
<li><code>sin(2*pi*Time) + cos(2*pi*Time)</code> is a fixed effect that captures seasonality.</li>
<li>The temporal covariance, is specified using the <code>correlations=</code> argument.</li>
<li>AR(1) was assumed by calling <code>correlation=corAR1()</code>. See <code>nlme::corClasses</code> for a list of supported correlation structures.</li>
<li>From the summary, we see that a <code>Mare</code> random effect has also been added. Where is it specified? It is implied by the <code>random=</code> argument. Read <code>?lme</code> for further details.</li>
</ul>
<p>We can now inspect the contrivance implied by our model’s specification.
As expected, we see the blocks of non-null covariance within <code>Mare</code>, but unlike “vanilla” LMMs, the covariance within mare is not fixed. Rather, it decays geometrically with time.</p>
<p><img src="Rcourse_files/figure-html/unnamed-chunk-206-1.png" width="50%" /></p>
</div>
<div id="extensions-1" class="section level2">
<h2><span class="header-section-number">8.4</span> Extensions</h2>
<div id="cr-se" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Cluster Robust Standard Errors</h3>
<p>As previously stated, random effects are nothing more than a convenient way to specify covariances within a level of a random effect, i.e., within a group/cluster.
This is also the motivation underlying <em>cluster robust</em> inference, which is immensely popular with econometricians, but less so elsewhere.
With cluster robust inference, we assume a model of type <span class="math inline">\(y=f(x)+\varepsilon\)</span>; unlike LMMs we assume independence (conditional on <span class="math inline">\(x\)</span>), but we allow <span class="math inline">\(\varepsilon\)</span> within clusters defined by <span class="math inline">\(x\)</span>.
For a longer comparison between the two approaches, see <a href="https://m-clark.github.io/docs/clustered/">Michael Clarck’s guide</a>.</p>
</div>
<div id="linear-models-for-panel-data" class="section level3">
<h3><span class="header-section-number">8.4.2</span> Linear Models for Panel Data</h3>
<p><strong>nlme</strong> and <strong>lme4</strong> will probably provide you with all the functionality you need for panel data.
If, however, you are trained as an econometrician, and prefer the econometric parlance, then the <a href="https://cran.r-project.org/package=plm">plm</a> and <a href="https://www.jacob-long.com/post/panelr-intro/">panelr</a> packages for panel linear models, are just for you.
In particular, they allow for cluster-robust covariance estimates, and Durbin–Wu–Hausman test for random effects.
The <strong>plm</strong> <a href="https://cran.r-project.org/web/packages/plm/vignettes/plm.pdf">package vignette</a> also has an interesting comparison to the <strong>nlme</strong> package.</p>
</div>
<div id="testing-hypotheses-on-correlations" class="section level3">
<h3><span class="header-section-number">8.4.3</span> Testing Hypotheses on Correlations</h3>
<p>After working so hard to model the correlations in observation, we may want to test if it was all required.
Douglas Bates, the author of <strong>nlme</strong> and <strong>lme4</strong> wrote a famous cautionary note, <a href="https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html">found here</a>, on hypothesis testing in mixed models, in particular hypotheses on variance components.
Many practitioners, however, did not adopt Doug’s view.
Many of the popular tests, particularly the ones in the econometric literature, can be found in the <strong>plm</strong> package (see Section 6 in the <a href="https://cran.r-project.org/web/packages/plm/vignettes/plm.pdf">package vignette</a>).
These include tests for poolability, Hausman test, tests for serial correlations, tests for cross-sectional dependence, and unit root tests.</p>
</div>
</div>
<div id="bibliographic-notes-6" class="section level2">
<h2><span class="header-section-number">8.5</span> Bibliographic Notes</h2>
<p>Most of the examples in this chapter are from the documentation of the <strong>lme4</strong> package <span class="citation">(Bates et al. <a href="#ref-lme4">2015</a>)</span>.
For a general and very applied treatment, see <span class="citation">Pinero and Bates (<a href="#ref-pinero2000mixed">2000</a>)</span>.
As usual, a hands on view can be found in <span class="citation">Venables and Ripley (<a href="#ref-venables2013modern">2013</a>)</span>, and also in an excellent blog post by <a href="http://rpsychologist.com/r-guide-longitudinal-lme-lmer">Kristoffer Magnusson</a>
For a more theoretical view see <span class="citation">Weiss (<a href="#ref-weiss2005modeling">2005</a>)</span> or <span class="citation">Searle, Casella, and McCulloch (<a href="#ref-searle2009variance">2009</a>)</span>.
Sometimes it is unclear if an effect is random or fixed; on the difference between the two types of inference see the classics: <span class="citation">Eisenhart (<a href="#ref-eisenhart1947assumptions">1947</a>)</span>, <span class="citation">Kempthorne (<a href="#ref-kempthorne1975fixed">1975</a>)</span>, and the more recent <span class="citation">Rosset and Tibshirani (<a href="#ref-rosset2018fixed">2018</a>)</span>.
For an interactive, beautiful visualization of the shrinkage introduced by mixed models, see <a href="http://m-clark.github.io/posts/2019-05-14-shrinkage-in-mixed-models/">Michael Clark’s blog</a>.
For more on predictions in linear mixed models see <span class="citation">Robinson (<a href="#ref-robinson1991blup">1991</a>)</span>, <span class="citation">Rabinowicz and Rosset (<a href="#ref-rabinowicz2018assessing">2018</a>)</span>, and references therein.
See <a href="https://m-clark.github.io/docs/clustered/">Michael Clarck’s</a> guide for various ways of dealing with correlations within groups.
For the geo-spatial view and terminology of correlated data, see <span class="citation">Christakos (<a href="#ref-christakos2000modern">2000</a>)</span>, <span class="citation">Diggle, Tawn, and Moyeed (<a href="#ref-diggle1998model">1998</a>)</span>, <span class="citation">Allard (<a href="#ref-allard2013j">2013</a>)</span>, and <span class="citation">Cressie and Wikle (<a href="#ref-cressie2015statistics">2015</a>)</span>.</p>
</div>
<div id="practice-yourself-4" class="section level2">
<h2><span class="header-section-number">8.6</span> Practice Yourself</h2>
<ol style="list-style-type: decimal">
<li><p>Computing the variance of the sample mean given dependent correlations. How does it depend on the covariance between observations? When is the sample most informative on the population mean?</p></li>
<li><p>Think: when is a paired t-test not equivalent to an LMM with two measurements per group?</p></li>
<li>Return to the <code>Penicillin</code> data set. Instead of fitting an LME model, fit an LM model with <code>lm</code>. I.e., treat all random effects as fixed.
<ol style="list-style-type: lower-alpha">
<li>Compare the effect estimates.</li>
<li>Compare the standard errors.</li>
<li>Compare the predictions of the two models.</li>
</ol></li>
<li>[Very Advanced!] Return to the <code>Penicillin</code> data and use the <code>gls</code> function to fit a generalized linear model, equivalent to the LME model in our text.</li>
<li>Read about the “oats” dataset using <code>? MASS::oats</code>.Inspect the dependency of the yield (Y) in the Varieties (V) and the Nitrogen treatment (N).
<ol style="list-style-type: decimal">
<li>Fit a linear model, does the effect of the treatment significant? The interaction between the Varieties and Nitrogen is significant?</li>
<li>An expert told you that could be a variance between the different blocks (B) which can bias the analysis. fit a LMM for the data.</li>
<li>Do you think the blocks should be taken into account as “random effect” or “fixed effect”?</li>
</ol></li>
<li><p>Return to the temporal correlation in Section <a href="lme.html#serial">8.3</a>, and replace the AR(1) covariance, with an ARMA covariance. Visualize the data’s covariance matrix, and compare the fitted values.</p></li>
</ol>
<p>See DataCamps’ <a href="https://www.datacamp.com/courses/hierarchical-and-mixed-effects-models">Hierarchical and Mixed Effects Models</a> for more self practice.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-allard2013j">
<p>Allard, Denis. 2013. “J.-P. Chiles, P. Delfiner: Geostatistics: Modeling Spatial Uncertainty.” Springer.</p>
</div>
<div id="ref-barr2013random">
<p>Barr, Dale J, Roger Levy, Christoph Scheepers, and Harry J Tily. 2013. “Random Effects Structure for Confirmatory Hypothesis Testing: Keep It Maximal.” <em>Journal of Memory and Language</em> 68 (3). Elsevier: 255–78.</p>
</div>
<div id="ref-lme4">
<p>Bates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using lme4.” <em>Journal of Statistical Software</em> 67 (1): 1–48. <a href="https://doi.org/10.18637/jss.v067.i01">https://doi.org/10.18637/jss.v067.i01</a>.</p>
</div>
<div id="ref-christakos2000modern">
<p>Christakos, George. 2000. <em>Modern Spatiotemporal Geostatistics</em>. Vol. 6. Oxford University Press.</p>
</div>
<div id="ref-cressie2015statistics">
<p>Cressie, Noel, and Christopher K Wikle. 2015. <em>Statistics for Spatio-Temporal Data</em>. John Wiley; Sons.</p>
</div>
<div id="ref-diggle1998model">
<p>Diggle, Peter J, JA Tawn, and RA Moyeed. 1998. “Model-Based Geostatistics.” <em>Journal of the Royal Statistical Society: Series C (Applied Statistics)</em> 47 (3). Wiley Online Library: 299–350.</p>
</div>
<div id="ref-eisenhart1947assumptions">
<p>Eisenhart, Churchill. 1947. “The Assumptions Underlying the Analysis of Variance.” <em>Biometrics</em> 3 (1). JSTOR: 1–21.</p>
</div>
<div id="ref-kempthorne1975fixed">
<p>Kempthorne, Oscar. 1975. “Fixed and Mixed Models in the Analysis of Variance.” <em>Biometrics</em>. JSTOR, 473–86.</p>
</div>
<div id="ref-pinero2000mixed">
<p>Pinero, Jose, and Douglas Bates. 2000. “Mixed-Effects Models in S and S-Plus (Statistics and Computing).” Springer, New York.</p>
</div>
<div id="ref-rabinowicz2018assessing">
<p>Rabinowicz, Assaf, and Saharon Rosset. 2018. “Assessing Prediction Error at Interpolation and Extrapolation Points.” <em>arXiv Preprint arXiv:1802.00996</em>.</p>
</div>
<div id="ref-robinson1991blup">
<p>Robinson, George K. 1991. “That Blup Is a Good Thing: The Estimation of Random Effects.” <em>Statistical Science</em>. JSTOR, 15–32.</p>
</div>
<div id="ref-rosset2018fixed">
<p>Rosset, Saharon, and Ryan J Tibshirani. 2018. “From Fixed-X to Random-X Regression: Bias-Variance Decompositions, Covariance Penalties, and Prediction Error Estimation.” <em>Journal of the American Statistical Association</em>, nos. just-accepted. Taylor &amp; Francis.</p>
</div>
<div id="ref-searle2009variance">
<p>Searle, Shayle R, George Casella, and Charles E McCulloch. 2009. <em>Variance Components</em>. Vol. 391. John Wiley &amp; Sons.</p>
</div>
<div id="ref-venables2013modern">
<p>Venables, William N, and Brian D Ripley. 2013. <em>Modern Applied Statistics with S-Plus</em>. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-weiss2005modeling">
<p>Weiss, Robert E. 2005. <em>Modeling Longitudinal Data</em>. Springer Science &amp; Business Media.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="16">
<li id="fn16"><p>Think: why bother treating the <code>Batch</code> effect as noise? Should we now just subtract <code>Batch</code> effects? This is not a trick question.<a href="lme.html#fnref16" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="glm.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multivariate.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/45-lme.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
